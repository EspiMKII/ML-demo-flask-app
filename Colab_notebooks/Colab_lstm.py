# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10hbC6OGkpt0fSW8YRcy002A788o7spoq

Model setup and model training should be run (in that order) before you run any of the other blocks.

# Model setup

Importing libraries
"""

import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, RobustScaler
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
from math import sqrt
import keras
import matplotlib.dates as mdates
import os

"""Setting parameters"""

ticker = 'GE'

train_percentage = 0.75
valid_percentage = 0.05

input_seq_length = 60
training_epochs = 5
training_batchsize = 32

output_prediction_length = 10

"""output_offset controls our N level (how many days ahead we are predicting).
N starts counting from 0: N = 1 => output_offset = 0, so on and so forth.
"""

output_offset = 0

"""Downloading data"""

df = yf.download(ticker, period='max')
close_prices = df.loc[:, 'Close']

"""Alternative for reading local data from txt file (txt file should be in a Stocks folder)"""

# df = pd.read_csv(os.path.join('Stocks', 'ge.us.txt'), delimiter=',', usecols=['Date', 'Open', 'High', 'Low', 'Close'])
# df = df.sort_values('Date')
# print(df)
# print(df.shape)
# close_prices = df.loc[:, 'Close']

"""Splitting into train - test - validation sets and reformatting data"""

train_breakpoint = round(len(close_prices) * train_percentage)
valid_breakpoint = round(len(close_prices) * (train_percentage + valid_percentage))

train_data = close_prices[:train_breakpoint]
valid_data = close_prices[train_breakpoint:valid_breakpoint]
test_data = close_prices[valid_breakpoint:]

training_set = train_data.values.reshape(-1, 1)
validation_set = valid_data.values.reshape(-1, 1)

"""Splitting into sequences for feeding into model"""

def create_sequences(data, seq_length):
    X = []
    Y = []
    for i in range(seq_length, len(data)):
        X.append(data[i - seq_length: i, 0])
        Y.append(data[i, 0])
    return np.array(X), np.array(Y)


X_train, Y_train = create_sequences(training_set, input_seq_length)
X_valid, Y_valid = create_sequences(validation_set, input_seq_length)

"""Scaling train and validation data - each datapoint is scaled individually."""

for v in range(len(X_train)):
    scaler_train = MinMaxScaler()
    X_train[v] = scaler_train.fit_transform(X_train[v].reshape(-1, 1)).reshape(-1)
    Y_train[v] = scaler_train.transform(Y_train[v].reshape(-1, 1)).reshape(-1)

for k in range(len(X_valid)):
    scaler_valid = MinMaxScaler()
    X_valid[k] = scaler_valid.fit_transform(X_valid[k].reshape(-1, 1)).reshape(-1)
    Y_valid[k] = scaler_valid.transform(Y_valid[k].reshape(-1, 1)).reshape(-1)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_valid = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))

"""# Model training

Training model.
"""

model = keras.Sequential()
model.add(keras.layers.LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.LSTM(units=50, return_sequences=True))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.LSTM(units=50, return_sequences=True))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.LSTM(units=50))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.Dense(units=1))

model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(X_train, Y_train, epochs=training_epochs, batch_size=training_batchsize, validation_data=(X_valid, Y_valid))

"""Plotting losses."""

plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.title('Model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""Printing out key error statistics (RMSE/MAPE). No $R^2$."""

print('RMSE: ' + str(sqrt(mean_squared_error(test_data.to_numpy().reshape(-1).tolist(), predicted_stock_price.reshape(-1).tolist()))))
print('MAPE: ' + str(mean_absolute_percentage_error(test_data.to_numpy().reshape(-1).tolist(), predicted_stock_price.reshape(-1).tolist())))

"""# Model prediction (for any value of N)

Processing test data - same steps as above.
"""

inputs = close_prices[len(close_prices) - len(test_data) - input_seq_length - output_offset:].values
inputs = inputs.reshape(-1, 1)
X_test = []
scaler_list = []
for i in range(input_seq_length + output_offset, len(inputs)):
    temp = MinMaxScaler()
    temp_array = inputs[i - input_seq_length - output_offset:i - output_offset, 0].reshape(-1, 1)
    X_test.append(temp.fit_transform(temp_array).reshape(-1))
    scaler_list.append(temp)
X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

"""Predicting on test data ([input_seq_length] days input => 1 day output)"""

predicted_stock_price = model.predict(X_test)
for v in range(len(predicted_stock_price)):
    predicted_stock_price[v] = scaler_list[v].inverse_transform(np.array(predicted_stock_price[v]).reshape(-1, 1)).reshape(-1)

"""Graphing predictions."""

date_start = df.index[valid_breakpoint]

date_range = pd.date_range(start=date_start, periods=len(predicted_stock_price), freq='B')

plt.figure(figsize=(10, 6))
plt.plot(date_range, test_data, color='blue', label='Stock price')
plt.plot(date_range, predicted_stock_price, color='red', label='Predicted stock price')
plt.gca().xaxis.set_major_locator(mdates.YearLocator(base=1))
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))
plt.title('Stock price prediction')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.show()

"""# Long-range prediction (compatible with only N = 1 for now)

Fetching data from the last [input_seq_length] days of the dataset - this will be our starting input.

We scale this input, then save the scaler. We will always need to keep the last scaler we used.
"""

initial_input = close_prices[len(close_prices) - input_seq_length:].values
initial_input = initial_input.reshape(-1, 1)

cur_scaler = MinMaxScaler()
initial_input = cur_scaler.fit_transform(initial_input)
cur_input = initial_input

cur_array = np.array(cur_input)
cur_array = np.reshape(cur_array, (1, cur_array.shape[0], cur_array.shape[1]))

"""The prediction loop.

The steps involved in one iteration of the loop are:


*   The model generates one prediction. That prediction is denormalized using the saved scaler, then saved in the output list.
*   The prediction is appended to the input, and the earliest stock data is removed from the input.
*   The input is renormalized using a new scaler (which we will now save) and reshaped. It is now ready to be fed into the model again.


"""

predict_prices = []

for _ in range(output_prediction_length):

    out_predict = model.predict(cur_array)
    out_price = cur_scaler.inverse_transform(out_predict.reshape(-1, 1)).reshape(-1).item()
    predict_prices.append([out_price])

    cur_input = cur_scaler.inverse_transform(cur_input)
    cur_list = cur_input.tolist()
    cur_list.pop(0)
    cur_list.append([out_price])
    cur_input = np.array(cur_list)

    cur_input.reshape(-1, 1)
    cur_scaler = MinMaxScaler()
    cur_input = cur_scaler.fit_transform(cur_input)

    cur_array = np.reshape(cur_input, (1, cur_input.shape[0], cur_input.shape[1]))

"""Visualizing our predictions."""

predict_range = pd.date_range(start=df.index[-1], periods=output_prediction_length, freq='B')

plt.figure(figsize=(10, 6))
plt.plot(predict_range, np.array(predict_prices), color='blue', label='Predictions')
plt.gca().xaxis.set_major_locator(mdates.MonthLocator())
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))
plt.title('Stock price prediction')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.show()

"""# Error results

Error for models predicting GE stock data taken from yfinance, N = 1.

|Epochs|$RMSE$|$MAPE$|
|-|-|-|
|1|2.9840|2.5732%|
|5|1.8750|1.5155%|
|10|1.7889|1.4674%|
|20|1.7995|1.4751%|

Error for models predicting GE stock data taken from Kaggle dataset, N = 1.

|Epochs|$RMSE$|$R^2$|$MAPE$|
|-|-|-|-|
|1|0.5189|0.9939|2.0114%|
|5|0.3443|0.9973|1.2678%|
|10|0.3427|0.9973|1.2445%|
|20|0.3411|0.9974|1.2565%|

Error for models predicting GE stock data taken from Kaggle dataset, N = 10.

|Epochs|$RMSE$|$R^2$|$MAPE$|
|-|-|-|-|
|1|1.1490|0.9702|4.6407%|
|5|1.0353|0.9758|4.1354%|
|10|1.0216|0.9764|4.0790%|
|20|1.0227|0.9764|4.0905%|

Error for models predicting GE stock data taken from Kaggle dataset, N = 30.

|Epochs|$RMSE$|$R^2$|$MAPE$|
|-|-|-|-|
|1|1.9586|0.9133|8.0556%|
|5|1.8995|0.9185|7.8037%|
|10|1.9236|0.9164|7.9313%|
|20|1.8157|0.9255|7.4458%|

Error for models predicting GOOGL stock data taken from Kaggle dataset, N = 1.

|Epochs|$RMSE$|$R^2$|$MAPE$|
|-|-|-|-|
|1|26.3263|0.9578|2.6380%|
|5|16.1503|0.9841|1.5145%|
|10|12.7019|0.9902|1.1972%|
|20|11.5633|0.9919|1.1064%|

Error for models predicting GOOGL stock data taken from Kaggle dataset, N = 10.

|Epochs|$RMSE$|$R^2$|$MAPE$|
|-|-|-|-|
|1|35.5634|0.9230|3.4142%|
|5|35.1852|0.9247|3.4434%|
|10|32.4659|0.9359|3.0915%|
|20|32.4800|0.9358|3.1554%|

Error for models predicting GOOGL stock data taken from Kaggle dataset, N = 30.

|Epochs|$RMSE$|$R^2$|$MAPE$|
|-|-|-|-|
|1|49.0062|0.8539|5.0842%|
|5|52.4592|0.8326|5.1449%|
|10|54.5687|0.8188|5.5179%|
|20|66.8843|0.7278|6.7308%|
"""