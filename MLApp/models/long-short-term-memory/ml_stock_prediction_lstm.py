# -*- coding: utf-8 -*-
"""ML - stock prediction - LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C-XenCS5Hu1zjIxeErbAwZytP-rp1xN7

Importing libraries.
"""

import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import keras
import matplotlib.dates as mdates
import os

"""Taking the data (Google Colab will not save the fucking file, please fetch the file from the data set or [here](https://drive.google.com/file/d/1KNH7VjZGjj5RAxlBRlcudCavGSoBNGmI/view?usp=sharing) and slam it in the sample_data folder)

Alternatively, you can change it to any other file in the Kaggle dataset and reallocate the number of data points for each of the three sets.

Alternatively alternatively, you can just fetch data from yfinance (I'm not doing it here because Google Colab is stupid and I get rate limited).
"""

df = pd.read_csv(os.path.join('sample_data', 'ge.us.txt'), delimiter=',', usecols=['Date', 'Open', 'High', 'Low', 'Close'])
df = df.sort_values('Date')
df = df.set_index('Date')
# print(df)
close_prices = df.loc[:, 'Close']

train_percentage = 0.85
valid_percentage = 0.05

input_seq_length = 60
training_epochs = 100
training_batchsize = 32

output_prediction_length = 365

#print(train_breakpoint)
#print(valid_breakpoint)

"""Separating the data into training, validation, and test sets.

The number of data points in each set are:

*   Training set: 10500 data points
*   Validation set: 1500 data points
*   Test set: 2058 data points
"""

train_breakpoint = round(len(close_prices) * train_percentage)
valid_breakpoint = round(len(close_prices) * (train_percentage + valid_percentage))

train_data = close_prices[:train_breakpoint]
valid_data = close_prices[train_breakpoint:valid_breakpoint]
test_data = close_prices[valid_breakpoint:]
# print(test_data)

training_set = train_data.values.reshape(-1, 1)
validation_set = valid_data.values.reshape(-1, 1)

"""Normalizing data. It may be better to normalize them in batches instead of normalizing as one big block like this - since earlier data will be squished very close to 0 and thus do not affect the model as much. Looking into this."""

scaler = MinMaxScaler(feature_range=(0, 1))
training_set = scaler.fit_transform(training_set)
validation_set = scaler.transform(validation_set)

"""This function reprocesses the data into chunks to feed into the Keras model. Chunk size is currently set to 20, i.e. the model will receive data from 20 previous days to predict any given day."""

def create_sequences(data, seq_length):
    X = []
    Y = []
    for i in range(seq_length, len(data)):
        X.append(data[i - seq_length: i, 0])
        Y.append(data[i, 0])
    return np.array(X), np.array(Y)


X_train, Y_train = create_sequences(training_set, input_seq_length)
X_valid, Y_valid = create_sequences(validation_set, input_seq_length)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_valid = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))

"""The actual model itself. It's a sequential model from the keras library, consisting of four layers of 50 LSTM nodes each. Might be a bit overkill tbh.

I'm using Adam as the optimizer and MSE as the loss calculation method.

The number of epochs can be adjusted to achieve a balance of speed and accuracy - I find that the model can already capture trends after 3-5 epochs, and after around 20, it starts to get really accurate.
"""

model = keras.Sequential()
model.add(keras.layers.LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.LSTM(units=50, return_sequences=True))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.LSTM(units=50, return_sequences=True))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.LSTM(units=50))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.Dense(units=1))

model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(X_train, Y_train, epochs=training_epochs, batch_size=training_batchsize, validation_data=(X_valid, Y_valid))

"""This is just for plotting out the loss over time.

Training loss down = good. Validation loss stable = good.
"""

plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.title('Model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""Prepping test data.

inputs is basically chopping the dataset to 20 days before the end of the test/validation sets (so that we have sufficient data for the first prediction of the test set) and then normalizing it.

Then we put all those chunks of 20-day data into X_test and reshape it to fit into the model.
"""

inputs = close_prices[len(close_prices) - len(test_data) - input_seq_length:].values
# print(inputs)
inputs = inputs.reshape(-1, 1)
inputs = scaler.transform(inputs)
X_test = []
for i in range(input_seq_length, len(inputs)):
    X_test.append(inputs[i - input_seq_length:i, 0])
X_test = np.array(X_test)
# print(X_test.shape)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
# print(X_test.shape)
# print(X_test)

"""Feeding X_test into the model for predictions, then reversing the outputs from normalized form into the actual prices."""

predicted_stock_price = model.predict(X_test)
# print(predicted_stock_price)
predicted_stock_price = scaler.inverse_transform(predicted_stock_price)

"""Visualizing results."""

date_start = df.index[valid_breakpoint]

date_range = pd.date_range(start=date_start, periods=len(predicted_stock_price), freq='B')

plt.figure(figsize=(10, 6))
plt.plot(date_range, test_data, color='blue', label='Stock price')
plt.plot(date_range, predicted_stock_price, color='red', label='Predicted stock price')
plt.gca().xaxis.set_major_locator(mdates.YearLocator(base=1))
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))
plt.title('Stock price prediction')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.show()

initial_input = close_prices[len(close_prices) - input_seq_length:].values
initial_input = initial_input.reshape(-1, 1)
pred_scaler = MinMaxScaler()
pred_scaler.fit(test_data.values.reshape(-1, 1))
initial_input = pred_scaler.transform(initial_input)

predict_input = [initial_input.tolist()]
# print(predict_input)
predict_array = np.array(predict_input)
# print(predict_array.shape)
predict_array = np.reshape(predict_array, (predict_array.shape[0], predict_array.shape[1], 1))

predict_output = []

for _ in range(output_prediction_length):
  out_price = model.predict(predict_array)
  # print(out_price.shape)
  predict_output.append([out_price[0][0].item()])
  predict_input[0].append([out_price[0][0].item()])
  predict_input[0].pop(0)
  # print(predict_input)
  predict_array = np.array(predict_input)
  predict_array = np.reshape(predict_array, (predict_array.shape[0], predict_array.shape[1], 1))

# print(predict_output)

predict_prices = pred_scaler.inverse_transform(predict_output)
# print(predict_prices)

predict_range = pd.date_range(start=df.index[-1], periods=output_prediction_length, freq='B')

plt.figure(figsize=(10, 6))
plt.plot(predict_range, predict_prices, color='blue', label='Predictions')
plt.gca().xaxis.set_major_locator(mdates.MonthLocator())
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))
plt.title('Stock price prediction')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.show()