# -*- coding: utf-8 -*-
"""ML - Tree-based Ensemble Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/115u0U2LnBQ5fcJ9EBeZdF45zi_d7hRkF

# Stock market predictions - Decision Trees and Random Forests

Welcome to my rather scuffed and, frankly, kinda shit implementation of the Decision Tree and Random Forest algorithms for stock market predictions.

The data used in this notebook is sourced from [this Kaggle dataset](https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs). Data from the GE stock ticker is used in specific, which can be found in the file "ge.us.txt" in CSV format. The dataset includes OHLC prices and the volume, all in 1-day intervals.

## Goals

The project's goal is to use the models above to predict the closing prices of a certain stock ticker. Namely, we want to predict the closing prices of the stocks over the next m days.

# Data preprocessing

## Importing modules
"""

from joblib import parallel_backend
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
import matplotlib.pyplot as plt # plotting
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error,r2_score
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.utils import parallel_backend
from xgboost import XGBRegressor
import seaborn as sns
import sklearn

"""# Utils"""

def evaluate(y_test, y_pred):
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    mape = mean_absolute_percentage_error(y_test, y_pred)

    print(f"Test RMSE: {rmse:.2f}")
    print(f"Test R2: {r2:.2f}")
    print(f"Test MAPE: {mape:.2f}")

def plot_prediction(y_pred, X_test):
    results = X_test.copy()
    results['Actual'] = y_test
    results['Predicted'] = y_pred
    results = results[['Actual', 'Predicted']]
    print(results.tail())

    plt.figure(figsize=(12, 6))
    plt.plot(results.index, results['Actual'], label='Actual', color='blue')
    plt.plot(results.index, results['Predicted'], label='Predicted', color='orange')
    plt.xlabel('Date')
    plt.ylabel('Stock Closing Price')
    plt.title('Actual vs Predicted Closing Prices')
    plt.legend()
    plt.tight_layout()
    plt.show()

def compare_train_test(model, X_train, X_test, y_train, y_test, model_name):
    y_train_pred = model.predict(X_train)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    train_r2 = r2_score(y_train, y_train_pred)
    train_mape = mean_absolute_percentage_error(y_train, y_train_pred)


    y_test_pred = model.predict(X_test)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_r2 = r2_score(y_test, y_test_pred)
    test_mape = mean_absolute_percentage_error(y_test, y_test_pred)

    print(f"{model_name} Performance:")
    print(f"Training RMSE: {train_rmse:.2f}, R2: {train_r2:.2f}, MAPE: {train_mape:.2f}")
    print(f"Test RMSE: {test_rmse:.2f}, R2: {test_r2:.2f}, MAPE: {test_mape:.2f}")
    print()

"""## Pulling data"""

df = pd.read_csv("https://github.com/noeruchangd/MLProject/blob/master/data/ge.us.txt?raw=true")

df.drop(['OpenInt','Volume'], axis=1, inplace=True)
df=df.tail(7500)

print(df)

"""# Preprocessing steps"""

df.describe()
df.set_index('Date', inplace=True)

print(df.dtypes)
# plt.figure(figsize = (10 , 5))
# sns.lineplot(df , x = df.index , y = 'Close')

"""Preliminary plotting"""

df['Close'].plot(title='GE Stock Closing Prices', figsize=(12, 6))
plt.xlabel('Date')

import statsmodels.api as sm # Shoutout to Chau for this!

# ACF for multiple lags
acf = sm.tsa.acf(df['Close'], nlags=20)

# Plot the ACF
plt.plot(range(1, 21), acf[1:])  # Start from lag=1
plt.title('ACF for GE Stock Price')
plt.xlabel('Lag')
plt.ylabel('Autocorrelation')
plt.xticks(range(1, 21))
plt.grid(True)
plt.show()

"""Feature engineering"""

# Basics
df['Average_price'] = (df['Close'] + df['Open']) / 2


# Moving averages
df['MA_5'] = df['Close'].rolling(window=5).mean()
df['MA_10'] = df['Close'].rolling(window=10).mean()
df['MA_20'] = df['Close'].rolling(window=20).mean()
# EMAs
df['EMA_5'] = df['Close'].rolling(window=5).mean()
df['EMA_10'] = df['Close'].rolling(window=10).mean()
df['EMA_20'] = df['Close'].rolling(window=20).mean()

df['STD_5'] = df['Close'].rolling(window=5).std()
df['STD_10'] = df['Close'].rolling(window=10).std()
df['STD_20'] = df['Close'].rolling(window=20).std()
# Lags
df['Close_t-1'] = df['Close'].shift(1)
df['Close_t-2'] = df['Close'].shift(2)
df['Close_t-3'] = df['Close'].shift(3)
#
df.describe()

corr = df.corr()

plt.figure(figsize=(20,12))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")

"""Splitting test and train sets"""

prediction_days = 10
df['Target'] = df['Close'].shift(-prediction_days)
df.dropna(inplace=True)

X = df.drop(columns=['Target'])
y = df['Target']

print(y.tail())

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, shuffle=False)

"""# Models

Decision tree (with only max_depth adjusted)
"""

# param_grid_DT = {
#     'max_depth': [10, 15, 20],
#     'min_samples_split': [2, 3, 5],
#     'min_samples_leaf': [1, 2, 3],
#     'max_features':  [None, 'log2', 'sqrt'],
#     'criterion': ['friedman_mse','squared_error'],
#     'ccp_alpha': [0.01, 0.015, 0.2]}

tree = DecisionTreeRegressor(ccp_alpha=0.01, criterion='friedman_mse', max_depth=20, max_features='log2', min_samples_leaf=2, min_samples_split=3)
tree.fit(X_train, y_train)

# print("Best Hyperparameters:", tree.best_params_)
# #
# print("Best Accuracy:", tree.best_score_)

y_pred = tree.predict(X_test)
evaluate(y_test, y_pred)
plot_prediction(y_pred, X_test)

"""## Random Forest"""

# param_grid_RF ={
#     'max_depth': [10, 15, 20],
#     'min_samples_split': [2, 3, 5],
#     'min_samples_leaf': [1, 2, 3],
#     'max_features':  ['log2', 'sqrt', None],
#     'criterion': ['friedman_mse', 'squared_error'],
#     'ccp_alpha': [0.01, 0.015, 0.02],
#     'n_estimators': [100, 250, 500],
#     'bootstrap': [True]
# }
# Best Hyperparameters: {'bootstrap': True, 'ccp_alpha': 0.02, 'criterion': 'squared_error', 'max_depth': 15, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}

forest = RandomForestRegressor(ccp_alpha=0.02, criterion='squared_error', max_depth=15, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100)
forest.fit(X_train, y_train)

# print("Best Hyperparameters:", forest.best_params_)
# print("Best Accuracy:", forest.best_score_)

y_pred = forest.predict(X_test)
evaluate(y_test, y_pred)
plot_prediction(y_pred, X_test)

"""Gradient Boosting"""

# param_grid_GBR ={
#     'max_depth': [10, 15, 20],
#     'n_estimators': [100, 200, 300],
#     'min_samples_leaf': [25, 40, 60],
#     'learning_rate': [0.2, 0.3, 0.4],
#     'ccp_alpha': [0.01, 0.015, 0.2],
#     'criterion': ['friedman_mse', 'squared_error']
# }
# Best Hyperparameters: {'ccp_alpha': 0.01, 'criterion': 'friedman_mse', 'learning_rate': 0.3, 'max_depth': 10, 'min_samples_leaf': 40, 'n_estimators': 300}
grad = GradientBoostingRegressor(ccp_alpha=0.01, criterion='friedman_mse', learning_rate=0.3, max_depth=10, min_samples_leaf=40,n_estimators=300)

grad.fit(X_train, y_train)

# print("Best Hyperparameters:", grad.best_params_)
# print("Best Accuracy:", grad.best_score_)

y_pred = grad.predict(X_test)
evaluate(y_test, y_pred)
plot_prediction(y_pred, X_test)

"""XGBoost"""

# param_grid_XGB = {
#     'learning_rate': [0.2, 0.25, 0.3],
#     'n_estimators': [600, 800, 1000],
#     'max_depth': [3, 4, 5],
#     'min_child_weight': [1, 2, 3],
#     'gamma': [0.1, 0.15, 0.2],
#     'subsample': [1.0],
#     'colsample_bytree': [0.9, 0.92, 0.95],
# }
# Best Hyperparameters: {'colsample_bytree': 0.95, 'gamma': 0.2, 'learning_rate': 0.2, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 600, 'subsample': 1.0}

xgb_cv = XGBRegressor(colsample_bytree=0.95, gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=3, n_estimators=600, subsample=1.0)

xgb_cv.fit(X_train, y_train)

# print("Best Hyperparameters:", xgb_cv.best_params_)
# print("Best Accuracy:", xgb_cv.best_score_)

y_pred = xgb_cv.predict(X_test)
evaluate(y_test, y_pred)
plot_prediction(y_pred, X_test)
# compare_train_test(xgb_cv, X_train, X_test, y_train, y_test, "XGBoost")

"""Voting Regressor"""

vr = VotingRegressor([('gbr', grad),
                      ('xgb', xgb_cv),
                      ('rf', forest)],
                    weights=[2,1,3])
vr.fit(X_train, y_train)
y_pred = vr.predict(X_test)
evaluate(y_test, y_pred)
plot_prediction(y_pred, X_test)

"""Overfitting test (comparing performance between testing and training sets)"""

compare_train_test(tree, X_train, X_test, y_train, y_test, "Decision Tree")
compare_train_test(forest, X_train, X_test, y_train, y_test, "Random Forest")
compare_train_test(grad, X_train, X_test, y_train, y_test, "Gradient Boosting")
compare_train_test(xgb_cv, X_train, X_test, y_train, y_test, "XGBoost")
compare_train_test(vr, X_train, X_test, y_train, y_test, "Voting Regressor")

"""Downloading the models for the demo"""

from joblib import dump

# Decision Tree
dump(tree, "decision-tree.pkl")
# Random Forest
dump(forest, "random-forest.pkl")
# Gradient Boosting
dump(grad, "gradient-boosting.pkl")
# XGBoost
dump(xgb_cv, "xgboosting.pkl")
# Voting Regressor
dump(vr, "voting-regressor.pkl")

from google.colab import files
files.download("decision-tree.pkl")
files.download("random-forest.pkl")
files.download("gradient-boosting.pkl")
files.download("xgboosting.pkl")
files.download("voting-regressor.pkl")