# -*- coding: utf-8 -*-
"""ML - Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13enpDmFHoL9pVJx7SLlA-T9wMXBmhuVs

# Preprocessing
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import yfinance as yf
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_percentage_error
from typing import Literal

sns.set_theme()

"""Download the dataset"""

df = yf.download('GOOG', interval='1d')
ge_df = yf.download('GE', interval='1d')

type(df)

if len(df) == 0:
    df = pd.read_csv("https://github.com/Tatsnien/ML-project/blob/main/GOOG.csv?raw=true")
    df['Date'] = pd.to_datetime(df['Date'])
    df.set_index('Date', inplace=True)

    ge_df = pd.read_csv("https://github.com/Tatsnien/ML-project/blob/main/GE.csv?raw=true")
    ge_df['Date'] = pd.to_datetime(ge_df['Date'])
    ge_df.set_index('Date', inplace=True)

df.head()

df.info()

df['Close'].plot(title='GOOG Stock Price', figsize=(10, 5))
plt.xlabel('Date')

ge_df['Close'].plot(title='GE Stock Price', figsize=(10, 5))
plt.xlabel('Date')

"""Prove the high autocorrelation of stock data to show that using past records is a good choice for training. As the graph has shown, recent records has very high autocorrelation with the current close value (\~1.0), and gradually decreases as the lag increase. At lag=20, the autocorrelation is still high (\~0.984)."""

import statsmodels.api as sm

# ACF for multiple lags
acf = sm.tsa.acf(df['Close'], nlags=20)

# Plot the ACF
plt.plot(range(1, 21), acf[1:])  # Start from lag=1
plt.title('ACF for Google Stock Price')
plt.xlabel('Lag')
plt.ylabel('Autocorrelation')
plt.xticks(range(1, 21))
plt.grid(True)
plt.show()

"""# Utils

Some predefined functions which will be moved to utils.py file.  
I have created a new function `create_dataset` for the fact that using `train_test_split` can enable the model to fit future trend while predicting past values, which is kinda dumb.
"""

def create_dataset(df, window=1, predicted_interval=1, fillna=False, size: None|int=None) -> tuple:
    """
    Create a dataset for time series forecasting by getting the previous window
    time steps as input features and the next predicted_interval time steps
    Args:
        df (pd.DataFrame): DataFrame containing stock data.
        window (int): Number of previous time steps to use as input features.
        predicted_interval (int): Number of time steps to predict.
        fillna (bool): Whether to fill NaN values in the dataset by interpolation with 'time' method.
        size (int, optional): If specified, randomly sample this number of rows from the dataset.
    Returns:
        tuple: A tuple containing the input features (X) and target variable (y).
        X (pd.DataFrame): Input features for the model.
        y (pd.Series): Target variable for the model.
    """

    pseudo_df = df.copy()
    pseudo_df = pseudo_df.asfreq('D')

    # Maximum number of rows in the dataset
    n = len(pseudo_df['Close']) - window - predicted_interval + 1
    if len(pseudo_df) < n or window < 1 or predicted_interval < 1 or window > len(pseudo_df) - predicted_interval:
        raise ValueError(f"You messed up the parameters!\n")

    X = pd.DataFrame(
        [pseudo_df['Close'].iloc[i:i + window].values.flatten() for i in range(n)],
        index=pseudo_df.index[window - 1:window - 1 + n],
        columns=[f'lag_{window - j}' for j in range(window)]
    )
    if fillna:
        X = X.interpolate(method='time')
    X['target'] = pseudo_df['Close'].shift(-predicted_interval).iloc[window - 1:window - 1 + n]
    if size and len(X) > size:
        X = X.sample(n=size, random_state=42)
    X = X.dropna()
    y = X.pop('target')

    return X, y

def split_dataset(X, y, test_size=0.2, method: Literal['half', 'cv']='half'):
    """
    Split the dataset into training and testing sets.
    Args:
        X (pd.DataFrame): Features.
        y (pd.Series): Target.
        test_size (float): Proportion of test set to the dataset.
        method (str): ['half', 'cv'] Method for splitting the dataset. \n
        'half' - Split the dataset 2 parts. \n
        'cv' - Split the dataset into k folds for cross-validation.
        Returns:
        tuple:
        X_train, X_test, y_train, y_test DataFrames if method='half'.
        X_trains, X_tests, y_trains, y_tests lists of DataFrames if method='cv'.
    """
    if test_size <= 0 or test_size >= 1:
        raise ValueError(f"test_size must be between 0 and 1, got {test_size}.\n")

    if method == 'half':
        mid = int(len(X) * (1 - test_size))
        X_train, X_test = X[:mid], X[mid:]
        y_train, y_test = y[:mid], y[mid:]
        return X_train, X_test, y_train, y_test
    elif method == 'cv':
        folds = int(1 / test_size)
        start = 0
        end = int(len(X) * test_size)
        fold_size = int(len(X) * test_size)
        X_trains, X_tests, y_trains, y_tests = [], [], [], []
        for i in range(folds):
            X_trains.append(X.iloc[start:end])
            y_trains.append(y.iloc[start:end])
            start = end
            end = min(end + fold_size, len(X))
        X_tests = X_trains[1:]
        y_tests = y_trains[1:]
        X_trains = X_trains[:-1]
        y_trains = y_trains[:-1]
        return X_trains, X_tests, y_trains, y_tests

def APE(y_test: pd.Series, y_pred: pd.Series) -> np.ndarray:
    np_y_test = y_test.to_numpy()
    np_y_pred = y_pred.to_numpy()
    return np.abs((np_y_test - np_y_pred) / np_y_test) * 100

def MAPE(y_test: pd.Series, y_pred: pd.Series) -> float:
    """
    Calculate Mean Absolute Percentage Error (MAPE).
    Args:
        y_test (pd.Series): True values.
        y_pred (pd.Series): Predicted values.
    Returns:
        float: MAPE value.
    """
    return np.mean(APE(y_test, y_pred))

def sMAPE(y_test: pd.Series, y_pred: pd.Series) -> float:
    """
    Calculate Symmetric Mean Absolute Percentage Error (sMAPE).
    Args:
        y_test (pd.Series): True values.
        y_pred (pd.Series): Predicted values.
    Returns:
        float: sMAPE value.
    """
    np_y_test = y_test.to_numpy()
    np_y_pred = y_pred.to_numpy()
    return np.mean(np.abs(np_y_test - np_y_pred) / ((np.abs(np_y_test) + np.abs(np_y_pred)) / 2)) * 100

def full_model_plot(compared_df, model_name="model") -> None:
    """
    Plot the results of the model as 3 figures:
    1. Predicted vs Actual
    2. Average Percentage Error
    3. Combined of 1 and 2
    Args:
        compared_df (pd.DataFrame): df containing True and Predicted values.
        model_name (str): Name of the model.
    Returns:
        None
    """

    # Set up the color palette
    palette_name = 'viridis'
    n_colors = 3
    palette = sns.color_palette(palette_name, n_colors)
    color_predict = palette[0]
    color_true = 'coral'
    color_error = palette[2]

    # Main plot (y_test vs y_pred)
    plt.figure(figsize=(10, 5))
    sns.lineplot(x=compared_df.index, y=compared_df['True'], label='True Values', color=color_true)
    sns.lineplot(x=compared_df.index, y=compared_df['Predicted'], label='Predicted Values', color=color_predict)
    plt.title(f'{model_name} Predictions')
    plt.xlabel('Sample Index')
    plt.ylabel('Stock Price')
    plt.legend()
    plt.show()

    # Combined plot (True vs Predicted and APE)
    fig, ax1 = plt.subplots(figsize=(12, 6))
    ax1.set_title(f'{model_name} Predictions and APE')
    ax1.set_xlabel('Sample Index')
    ax1.set_ylabel('Stock Price', color=color_true)
    sns.lineplot(x=compared_df.index, y=compared_df['True'], label='True Values', color=color_true)
    sns.lineplot(x=compared_df.index, y=compared_df['Predicted'], label='Predicted Values', color=color_predict)
    ax1.tick_params(axis='y', labelcolor=color_true)
    ax1.legend(loc='upper left')
    ax1.grid(False)

    # Create a twin y-axis for APE
    ax2 = ax1.twinx()
    ape = APE(compared_df['True'], compared_df['Predicted'])
    ax2.set_ylabel('APE (%)', color=color_error)
    # sns.lineplot(x=compared_df.index, y=pd.Series(ape), label='APE', ax=ax2, color=color_error, linestyle='--')
    ape_series = pd.Series(ape, index=compared_df.index)
    for date, _ in compared_df.iterrows():
        ax2.plot(
            [date, date],
            [0, ape_series[date]],
            color=color_error,
            alpha=0.5)
    ax2.tick_params(axis='y', labelcolor=color_error)
    ax2.legend([plt.Line2D([0], [0], color=color_error)], ['APE'], loc='upper right')
    ax2.grid(False)

    ax1.set_zorder(ax2.get_zorder() + 1)
    ax1.patch.set_visible(False)

    fig.tight_layout()
    plt.show()

def get_result(pipeline, compared_df) -> dict:
    """
    Get the result of the model.
    Args:
        model (sklearn model): Trained model.
        compared_df (pd.DataFrame): DataFrame containing True and Predicted values.
    Returns:
        dict: key=['Result', 'Pipeline', 'Coef', 'Intercept', 'RMSE', 'R^2', 'MAPE', 'sMAPE']
    """

    rmse = root_mean_squared_error(compared_df['True'], compared_df['Predicted'])
    r2 = r2_score(compared_df['True'], compared_df['Predicted'])
    mape= MAPE(compared_df['True'], compared_df['Predicted'])
    smape = sMAPE(compared_df['True'], compared_df['Predicted'])

    return {
        'Result': compared_df,
        'Pipeline': pipeline,
        'Coef': pipeline.named_steps['model'].coef_,
        'Intercept': pipeline.named_steps['model'].intercept_,
        'RMSE': rmse,
        'R^2': r2,
        'MAPE': mape,
        'sMAPE': smape,
    }

def get_compared_df(pipeline, X, y) -> pd.DataFrame:
    """
    Get the compared DataFrame.
    Args:
        X (pd.DataFrame): Features.
        y (pd.Series): True values.
    Returns:
        pd.DataFrame: (True, Predicted).
    """

    return pd.DataFrame({
        'True': y,
        'Predicted': pd.Series(pipeline.predict(X).flatten(), index=X.index)
        }, y.index)

def model_frame(model, X_train, X_test, y_train, y_test, show_plot=True, train_size=None) -> dict:
    """
    Train a model and evaluate its performance.
    Args:
        model: The machine learning model to train.
        X_train (pd.DataFrame): Training features.
        y_train (pd.Series): Training target variable.
        X_test (pd.DataFrame): Testing features.
        y_test (pd.Series): Testing target variable.
        show_plot (bool): Whether to show the plot of the model's predictions.
    Returns:
        dict: Dict containing the model's predictions and performance metrics.
    """
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])

    if train_size and train_size < len(X_train):
        X_train = X_train.tail(train_size)
        y_train = y_train.tail(train_size)

    pipeline.fit(X_train, y_train)

    compared_df = get_compared_df(pipeline, X_test, y_test)
    result = get_result(pipeline, compared_df)

    print(f'{type(model).__name__} RMSE: {result["RMSE"]:.4f}, R^2: {result["R^2"]:.4f}\n')
    print(f'Coef: {result["Coef"]}')
    print(f'MAPE: {result["MAPE"]:.4f}%')
    print(f'Max APE: {np.max(APE(result["Result"]["True"], result["Result"]["Predicted"])):.4f}%')
    print(f'sMAPE: {result["sMAPE"]:.4f}%')
    print()

    if show_plot:
        full_model_plot(compared_df, type(model).__name__)

    return result

def cv_model_frame(model, X_trains, X_tests, y_trains, y_tests, show_plot=True, train_size=None) -> dict:
    """
    Iteratively take 1 fold for train and the next fold for test.
    Find average results.
    Args:
        model: The machine learning model to train. *e.g. LinearRegression()*
        X_trains (list): List of training features for each fold.
        y_trains (list): List of training target variable for each fold.
        X_tests (list): List of testing features for each fold.
        y_tests (list): List of testing target variable for each fold.
    Return:
        dict:
        'Result' (list): List of each fold's result.
        'AvgRMSE' (float)
        'AvgR^2' (float)
        'AvgMAPE' (float)
        'AvgsMAPE' (float)
    """
    results = {
        'AllResults' : [],
        'AvgRMSE' : None,
        'AvgR^2' : None,
        'AvgMAPE' : None,
        'AvgsMAPE' : None
    }

    for i in range(len(X_trains)):
        print(f'Fold {i + 1}/{len(X_trains)}')
        result = model_frame(model, X_trains[i], X_tests[i], y_trains[i], y_tests[i], show_plot=show_plot, train_size=train_size)
        results['AllResults'].append(result)

    results['AvgRMSE'] = np.mean([result['RMSE'] for result in results['AllResults']])
    results['AvgR^2'] = np.mean([result['R^2'] for result in results['AllResults']])
    results['AvgMAPE'] = np.mean([result['MAPE'] for result in results['AllResults']])
    results['AvgsMAPE'] = np.mean([result['sMAPE'] for result in results['AllResults']])
    results['StdRMSE'] = np.std([result['RMSE'] for result in results['AllResults']])
    results['StdR^2'] = np.std([result['R^2'] for result in results['AllResults']])
    results['StdMAPE'] = np.std([result['MAPE'] for result in results['AllResults']])
    results['StdsMAPE'] = np.std([result['sMAPE'] for result in results['AllResults']])
    print(f'Avg RMSE: {results["AvgRMSE"]:.4f} +/- {results["StdRMSE"]:.4f}')
    print(f'Avg R^2: {results["AvgR^2"]:.4f} +/- {results["StdR^2"]:.4f}')
    print(f'Avg MAPE: {results["AvgMAPE"]:.4f}% +/- {results["StdMAPE"]:.4f}%')
    print(f'Avg sMAPE: {results["AvgsMAPE"]:.4f}% +/- {results["StdsMAPE"]:.4f}%')

    return results

def finetune_frame(model, X_train, X_test, y_train, y_test, param_grid, cv=10) -> pd.DataFrame:
    """
    Fine-tune the model using GridSearchCV.
    Args:
        model: The machine learning model to train.
        X_train (pd.DataFrame): Training features.
        y_train (pd.DataFrame): Training target variable.
        X_test (pd.DataFrame): Testing features.
        y_test (pd.DataFrame): Testing target variable.
    Returns:
        dict: Dict containing the model's predictions and performance metrics.
    """
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])

    kf = KFold(n_splits=cv, shuffle=False)
    custom_folds = [(train_index.tolist(), test_index.tolist()) for train_index, test_index in kf.split(X_train)]

    grid_search = GridSearchCV(pipeline, param_grid, cv=custom_folds, scoring='neg_mean_squared_error', n_jobs=-1)
    grid_search.fit(X_train, y_train)

    best_pipeline = grid_search.best_estimator_
    best_params = grid_search.best_params_

    compared_df = get_compared_df(best_pipeline, X_test, y_test)
    result = get_result(best_pipeline, compared_df)
    result['GridSearch'] = grid_search

    print(f'{type(model).__name__} RMSE: {result["RMSE"]:.4f}, R^2: {result["R^2"]:.4f}\n')
    print(f'Coef: {result["Coef"]}')
    print(f'MAPE: {result["MAPE"]:.4f}%')
    print(f'Max APE: {np.max(APE(result["Result"]["True"], result["Result"]["Predicted"])):.4f}%')
    print(f'sMAPE: {result["sMAPE"]:.4f}%')

    return result

"""# Methods

## Linear Regression
"""

X, y = create_dataset(df, window=2, predicted_interval=5)
X_train, X_test, y_train, y_test = split_dataset(X, y, test_size=0.2, method='half')

X.head(10)

lr = model_frame(LinearRegression(), X_train, X_test, y_train, y_test)

lr

X_trains, X_tests, y_trains, y_tests = split_dataset(X, y, test_size=0.2, method='cv')
lr_cv = cv_model_frame(LinearRegression(), X_trains, X_tests, y_trains, y_tests)

plt.figure(figsize=(10, 6))
sns.scatterplot(x=lr['Result']['True'], y=lr['Result']['Predicted'], label='Predicted vs True', color='blue', s=10)
plt.plot([lr['Result']['True'].min(), lr['Result']['True'].max()],
         [lr['Result']['True'].min(), lr['Result']['True'].max()],
         color='red', label='Ideal Prediction', lw=0.5)
plt.title('Regression Line for GOOG Linear Regression')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.legend()
plt.gca().set_aspect('equal', adjustable='box')
plt.grid(True)
plt.show()

# Overall results
overall_result = get_result(lr['Pipeline'], get_compared_df(lr['Pipeline'], X, y))
overall_result

"""### Choose window size

Window size = 2 is optimal.
"""

# rmse_list = []
# mape_list = []
# window_list = [1, 2, 3, 4, 5, 6, 7, 10, 15, 20, 30, 50]

# # for i in window_list:
# #     print(f'Window: {i}')
# #     rmse_temp = []
# #     mape_temp = []
# #     for _ in range(5):
# #         X, y = create_dataset(df, window=i, predicted_interval=5, fillna=True, size=1000)
# #         X_train, X_test, y_train, y_test = split_dataset(X, y, test_size=0.2, method='half')
# #         lr = model_frame(LinearRegression(), X_train, X_test, y_train, y_test, show_plot=False)
# #         rmse_temp.append(lr['RMSE'])
# #         mape_temp.append(lr['MAPE'])
# #     rmse_list.append(np.mean(rmse_temp))
# #     mape_list.append(np.mean(mape_temp))
# #     print('----------------------------------------')
# # plt.show()

# # A inappropriated version, but faster than the above
# for i in window_list:
#     print(f'Window: {i}')
#     X, y = create_dataset(df, window=i, predicted_interval=30, fillna=True, size=1000)
#     # X_train, X_test, y_train, y_test = split_dataset(X, y, test_size=0.2, method='half')
#     # lr = model_frame(LinearRegression(), X_train, X_test, y_train, y_test, show_plot=False)
#     X_trains, X_tests, y_trains, y_tests = split_dataset(X, y, test_size=0.2, method='cv')
#     lr_cv = cv_model_frame(LinearRegression(), X_trains, X_tests, y_trains, y_tests)
#     rmse_list.append(lr_cv['AvgRMSE'])
#     mape_list.append(lr_cv['AvgMAPE'])
#     print('----------------------------------------')
# plt.show()

# plt.figure(figsize=(8, 4))
# plt.subplot(1, 2, 1)
# plt.plot([str(i) for i in window_list], rmse_list, lw=1.0)
# plt.title('RMSE vs Window Size')
# plt.xlabel('Window Size')
# plt.ylabel('RMSE')
# plt.subplot(1, 2, 2)
# plt.plot([str(i) for i in window_list], mape_list, lw=1.0)
# plt.title('MAPE vs Window Size')
# plt.xlabel('Window Size')
# plt.ylabel('MAPE (%)')
# plt.tight_layout()
# plt.show()

"""## Ridge, Lasso Regression"""

ridge = finetune_frame(Ridge(), X_train, X_test, y_train, y_test, param_grid={
    'model__alpha': [0.1, 1.0, 10.0],
    'model__solver': ['auto', 'saga', 'sparse_cg']
})
print('-----------------------------')
lasso = finetune_frame(Lasso(), X_train, X_test, y_train, y_test, param_grid={
    'model__alpha': [0.1, 1.0, 10.0]
})

"""Because the model is not overfitting, both Ridge and Lasso' best model choose the smallest possible parameter."""

full_model_plot(ridge['Result'], 'Ridge Regression')
print(ridge['GridSearch'].best_params_)
print(ridge['GridSearch'].best_estimator_.named_steps['model'].coef_)
print(ridge)
full_model_plot(lasso['Result'], 'Lasso Regression')
print(lasso['GridSearch'].best_params_)
print(lasso['GridSearch'].best_estimator_.named_steps['model'].coef_)
print(lasso)

"""## Support Vector Regression"""

param_grid = {
    'model__C': [0.1, 1, 10, 100],
    'model__kernel': ['linear', 'rbf', 'poly', 'sigmoid'],
    'model__epsilon': [0.01, 0.1, 0.2],
    'model__gamma': ['scale', 'auto']
}

# param_grid = {
#     'model__C': [1, 100, 500, 1000],
#     'model__kernel': ['linear'],
#     'model__epsilon': [0.01, 0.0]
# }

svr = finetune_frame(SVR(), X_train, X_test, y_train, y_test, param_grid=param_grid)

"""{'model__C': 10, 'model__epsilon': 0.01, 'model__gamma': 'scale', 'model__kernel': 'linear'}  
Coef: [[ 1.49208457 20.13125626]]  
Intercept: [28.24812603]  
RMSE: 4.1804  
R^2: 0.9757  
MAPE: 2.4357%  
sMAPE: 2.4274%  
"""

print(svr['GridSearch'].best_params_)
print(f"Coef: {svr['Coef']}")
print(f"Intercept: {svr['Intercept']}")
print(f"RMSE: {svr['RMSE']:.4f}")
print(f"R^2: {svr['R^2']:.4f}")
print(f"MAPE: {svr['MAPE']:.4f}%")
print(f"sMAPE: {svr['sMAPE']:.4f}%")
full_model_plot(svr['Result'], 'SVR')

"""# Testing on GE dataset"""

ge_X, ge_y = create_dataset(ge_df, window=2, predicted_interval=5)
ge_X_train, ge_X_test, ge_y_train, ge_y_test = split_dataset(ge_X, ge_y, test_size=0.2, method='half')

ge_lr = model_frame(LinearRegression(), ge_X_train, ge_X_test, ge_y_train, ge_y_test)

ge_lr

plt.figure(figsize=(10, 6))
sns.scatterplot(x=ge_lr['Result']['True'], y=ge_lr['Result']['Predicted'], label='Predicted vs True', color='blue', s=10)
plt.plot([ge_lr['Result']['True'].min(), ge_lr['Result']['True'].max()],
         [ge_lr['Result']['True'].min(), ge_lr['Result']['True'].max()],
         color='red', label='Ideal Prediction', lw=0.5)
plt.title('Regression Line for GE Linear Regression')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.legend()
plt.gca().set_aspect('equal', adjustable='box')
plt.grid(True)
plt.show()